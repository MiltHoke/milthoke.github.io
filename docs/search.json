[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/webscraping/index.html",
    "href": "posts/webscraping/index.html",
    "title": "Scraping web data with R and Docker",
    "section": "",
    "text": "Introduction\nI’ve spent a considerable amount of time sifting through tutorials on how to scrape data from the web. Both R and Python offer tools to easily parse html data but so far the only easy solution I’ve found to scrape dynamic data rendered though JS (maybe it’s JS?, either way it shows up magicially on the screen but isn’t in the html to parse) is in R.\nFor this little project we will be recording the building capacity counts for the UW Madison Rec. Inside this site there are two pages we’ll want to scrape. The first page is the overall building capactiy at https://services.recwell.wisc.edu/FacilityOccupancy and a more granular look at specific area capactiy in https://recwell.wisc.edu/liveusage/.\nLastly, I don’t remember what I’ve installed onto my computer to make this all run, but you don’t need to worry about that because we’ll also put together a docker image you can build to easily run both of the examples below.\n\n\nUsing rvest (EASY MODE)\nFirst we’ll start with scraping overall building occupancy. For this we just need to grab the HTML from the URL https://services.recwell.wisc.edu/FacilityOccupancy and strip out the numbers that we want using their xpaths.\nlibrary(rvest)\nlibrary(tidyverse)\nurlPath &lt;- 'https://services.recwell.wisc.edu/FacilityOccupancy'\nhtml &lt;- read_html(urlPath) \n\n#output of html\n&gt; html\n{html_document}  \n&lt;html lang=\"en-US\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt;\\n&lt;link rel=\"icon\" href=\"~/favicon.ico\"&gt;\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"v \n[2] &lt;body&gt;\\r\\n    &lt;div role=\"complementary\" aria-label=\"skip to main content\"&gt;&lt;a id=\"skipLink\" class=\"skip-main\" tabindex=\"1\" onclick=\"$('#mainContent').find( \nNow we have a list of html text stored in the variable html (output shown above). Inside of html are the two numbers we want to extract: Current Occupancy and max Occupancy. First you need to find the xpaths (or something similiar) for each occupancy value you want to pull out. If you don’t know how to do this just search for ‘finding xpath in chrome’ and you’re sure to find something. Once you have the xpath for the values you want the rest is pretty straightforward.\noccupancy &lt;- \n  html %&gt;%\n  html_nodes(xpath='//*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[3]/strong | //*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[1]/strong') %&gt;%\n  html_text() %&gt;% \n  str_replace(\"%\",\"\") %&gt;% \n  as.numeric() \n  \n&gt; occupancy\n[1] 222  73\nIn the code above, html_nodes() will extract the values we want from the html based on the xpaths we gave it to look for. This value will still have html tags on it (e.g. &lt;strong&gt; 222 &lt;/strong&gt;) which we can remove using the html_text() function. From there it’s just some simple cleanup to remove special characters with str_replace() and convert them from character into numeric using as.numeric().\nFinally we can take our vector and turn it into a dataframe for additional manipulation.\ndf &lt;- data.frame(max_occupancy  = occupancy[1], current_occupancy = occupancy[2]/100, pulled = Sys.time())\n&gt; df\n  max_occupancy current_occupancy              pulled\n1           222              0.73 2020-10-28 19:35:19\n\n\nUsing pagedown and pdftools\nOK, now lets wrap this up and head over to https://recwell.wisc.edu/liveusage/, grab a few xpaths and be good to go. Unfortunatly this one isn’t quite as easy.\nurlPath &lt;- 'https://recwell.wisc.edu/liveusage/'\nhtml &lt;- read_html(urlPath)\n\nhtml %&gt;%\nhtml_nodes(xpath='//*[@id=\"nick\"]/div/div[2]/div[2]/div/div/div[1]/div/div[2]/p[2]/span[1]') %&gt;%\n  html_text()\n  \n[1] \"0\"\nThe code above will return 0 instead of 18 which is the correct value at the time I ran this example. I believe 0 is a placeholder and the actual value gets updated at a later time. Either way, it doesn’t look like the method we used above will work for us so we’ll have to shift gears a bit.\nThe most reliable way I’ve found to scrape this type of data is to use pagedown to print the page to a pdf. Then use pdftools to read the pdf back into R. Then use some regular expression magic to parse everything you need.\n#Set some variables\nlibrary(pdftools)\nlibrary(tidyverse)\nurlPath &lt;- 'https://recwell.wisc.edu/liveusage/'\npdfPath &lt;- \"./rec.pdf\"\n\n#Print out the webpage to pdf using pagedown\npagedown::chrome_print(urlPath,pdfPath,extra_args = '--no-sandbox')\n\n#Read in the pdf again using pdftools (and some stringr magic)\nx &lt;- str_squish(unlist(str_split(str_flatten(pdf_text(pdfPath)),\"\\n\")))\n\n#Use regular expressions to parse out the data we want to keep\nvalues_bool &lt;- str_detect(x,\"^Updated (.*?) \\\\d+ / \\\\d+\")\nlabel_bool &lt;- values_bool[c(2:length(values_bool),FALSE)]\n\n#Turn it into a dataframe for additional manipulation.\ndata.frame(location = x[label_bool],values = x[values_bool])\n\n\nMaking it easy with Docker\nYou can build your own image using the Dockerfile code below, or pull mine from docker hub at mjholt02/pagedown.\nFROM r-base\n\nRUN apt-get update -qq && apt-get -y install libssl-dev \\\n    chromium libcurl4-openssl-dev \\ \n    libxml2-dev libpoppler-cpp-dev libpq-dev\n\nRUN install2.r RPostgres pagedown pdftools tidyverse \\ \n    rvest\nFrom there is as easy as starting up an interactive container using docker run --rm -it mjholt02/pagedown or winpty docker run --rm -it mjholt02/pagedown if you’re using git bash."
  },
  {
    "objectID": "posts/step_adstock/index.html",
    "href": "posts/step_adstock/index.html",
    "title": "A tunable adstock step function",
    "section": "",
    "text": "Introduction\nI’ve been using the Tidymodels framework quite a bit lately and was interested in creating a tunable adstock function to search for the ideal rate without deviating from the Tidymodels framework. The functions below are pretty much just a rework of the ones provided in the documentation but extending it to include a tunable function input.\n\n\nCreating the functions\nBelow are the functions you’ll need to prep, bake, and tune your step function.\n\n### Specify the function to calculate Adstock given a spend vector\ncalcAdstock &lt;-function(x,w,args=NULL){\n  return(as.numeric(stats::filter(x=x,filter=w,method=\"recursive\")))\n}\n\nstep_adstock &lt;- \nfunction(recipe, ..., role=NA, trained=FALSE, skip=FALSE, \n  id=rand_id(\"adstock\"), my_vector=NULL, w=0){\n  \n  terms = ellipse_check(...)\n  \n  add_step(\n    recipe,\n    step_adstock_new(\n      terms=terms,\n      trained=trained,\n      role=role,\n      skip=skip,\n      id=id,\n      my_vector=my_vector,\n      w=w)\n  )\n}\n                         \nstep_adstock_new &lt;- \nfunction(terms, role, trained, my_vector,skip, id, w){\n  step(\n    subclass = \"adstock\",\n    terms=terms,\n    role=role,\n    trained=trained,\n    my_vector=my_vector,\n    skip=skip,\n    id=id,\n    w=w)\n}    \n\nprep.step_adstock &lt;- \nfunction(x,training,info=NULL, ...){\n  col_names &lt;- recipes_eval_select(x$terms, training, info)\n  \n  my_vector &lt;- purrr::map(training[, col_names],calcAdstock, w=x$w)\n  \n  step_adstock_new(\n    terms = x$terms,\n    trained=TRUE,\n    role=x$role,\n    my_vector = my_vector,\n    skip = x$skip,\n    id = x$id,\n    w=x$w)\n}\n\nbake.step_adstock &lt;- \nfunction(object, new_data, ...){\n  vars &lt;- names(object$my_vector)\n  \n  new_data[,vars]&lt;- \n    purrr::map2_dfc(new_data[,vars],object$my_vector, calcAdstock, w=object$w)\n  \n  tibble::as_tibble(new_data)\n}\n\ntunable.step_adstock &lt;- \nfunction(x, ...){\n  tibble::tibble(\n    name = c(\"w\"),\n    call_info = list(list(pkg=\"dials\",fun=\"mixture\",range=c(0,1))),\n    source = \"recipe\",\n    component = \"step_adstock\",\n    component_id = x$id)\n}\n\n\n\nExample\nFirst step is to simulate some data so our dependent variable (sales) is related to the lagged values of tv and radio spend.\n\n#install.packages(\"pacman\")\nset.seed(123)\npacman::p_load(tidyverse, tidymodels,lubridate,doParallel)\n\ndateVector &lt;- \n  seq.Date(as.Date(\"2018-01-01\"),as.Date(\"2021-12-31\"),1)\n\ndf &lt;- \n  tibble(date = dateVector, \n         tv = rnorm(length(dateVector),100,20), \n         radio = rnorm(length(dateVector),50,5)) %&gt;%\n  mutate(sales = round((lag(tv,1)*.075) + (lag(tv,2)*0.0375) + \n                 (lag(radio,1) *.01 )) ) %&gt;%\n  drop_na()\n\nhead(df)\n\n# A tibble: 6 × 4\n  date          tv radio sales\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2018-01-03 131.   53.2    11\n2 2018-01-04 101.   52.1    14\n3 2018-01-05 103.   49.0    13\n4 2018-01-06 134.   49.6    12\n5 2018-01-07 109.   53.4    14\n6 2018-01-08  74.7  50.9    14\n\n\nNext we split into our training/test sets. In this case we need to incorporate lagged values so when we make predictions on the test set we have the appropriate spend history to build up our adstock transformation properly for the first few observations.\n\nmy_split &lt;- initial_time_split(df,lag=8)\ntrain &lt;- training(my_split)\ntest &lt;- testing(my_split)\n\nmy_rs &lt;- \n  sliding_period(train, date, period = \"month\", \n                 lookback = 24, assess_stop = 4)\n\nUsing the tidymodels framework, lets setup, run/tune, and fit our 3 models.\n\ndoParallel::registerDoParallel()\n\nmdl_linear &lt;- \n  linear_reg() \n\nr_base &lt;- \n  recipe(sales ~ tv + radio, data = train) \n\nr_single &lt;- r_base %&gt;%\n  step_adstock(c('tv','radio'), w = tune('w1')) \n\nr_multi &lt;- r_base %&gt;%\n  step_adstock('tv', w = tune('w1')) %&gt;%\n  step_adstock('radio', w = tune('w2')) \n\n\nwkflw &lt;-\n  workflow() %&gt;%\n  add_model(mdl_linear)\n\nwkflw_base   &lt;- wkflw %&gt;% add_recipe(r_base)\nwkflw_single &lt;- wkflw %&gt;% add_recipe(r_single)\nwkflw_multi  &lt;- wkflw %&gt;% add_recipe(r_multi)\n\nfit_base &lt;- \n  wkflw_base %&gt;%\n  last_fit(my_split)\n\n#single adstock \nwkflw_single_t &lt;- \nwkflw_single %&gt;%\n  tune_grid(my_rs,grid=15)\n\nfit_single &lt;- \nwkflw_single %&gt;%\n  finalize_workflow(select_best(wkflw_single_t, metric = \"rmse\")) %&gt;%\n  last_fit(my_split) \n\n#multi adstock \nwkflw_multi_t &lt;- \nwkflw_multi %&gt;%\n  tune_grid(my_rs,grid=15)\n\nfit_multi &lt;- \nwkflw_multi %&gt;%\n  finalize_workflow(select_best(wkflw_multi_t, metric = \"rmse\")) %&gt;%\n  last_fit(my_split) \n\nfinal_base &lt;- extract_workflow(fit_base)\nfinal_single &lt;- extract_workflow(fit_single)\nfinal_multi &lt;- extract_workflow(fit_multi)\n\nExamine our model metrics and the tuned adstock rates.\n\nselect_best(wkflw_single_t, metric = \"rmse\")\n\n# A tibble: 1 × 2\n     w1 .config              \n  &lt;dbl&gt; &lt;chr&gt;                \n1 0.722 Preprocessor11_Model1\n\nselect_best(wkflw_multi_t, metric = \"rmse\")\n\n# A tibble: 1 × 3\n     w1    w2 .config              \n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                \n1 0.802 0.810 Preprocessor11_Model1\n\ncollect_metrics(fit_base)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    1.68     Preprocessor1_Model1\n2 rsq     standard    0.000614 Preprocessor1_Model1\n\ncollect_metrics(fit_single)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.42  Preprocessor1_Model1\n2 rsq     standard       0.293 Preprocessor1_Model1\n\ncollect_metrics(fit_multi)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       1.42  Preprocessor1_Model1\n2 rsq     standard       0.286 Preprocessor1_Model1\n\n\nPredict, summarize, and plot our predictions for each model. In the plot below I’ll summarize by quarter just to make it easier to visually see the differences between the models.\n\ntest_plot &lt;- test\n\ntest_plot$base &lt;- predict(final_base,test_plot) %&gt;% pull()\ntest_plot$single &lt;- predict(final_single,test_plot) %&gt;% pull()\ntest_plot$multi &lt;- predict(final_multi,test_plot) %&gt;% pull()\n\ntest_plot %&gt;%\n  select(date, sales:multi) %&gt;%\n  pivot_longer(cols = -date) %&gt;%\n  mutate(date = floor_date(date,\"quarter\")) %&gt;%\n  group_by(date,name) %&gt;%\n  summarise(value = sum(value)) %&gt;%\n  filter(date &gt;= \"2021-01-01\") %&gt;%\n  ggplot(aes(x=date, y=value, color = name)) + geom_line(linewidth=1.5) +\n  labs(x=NULL, y = \"Sales\", color = \"Model\") + \n  scale_color_manual(values=c(\"red\", \"blue\", \"black\", \"orange\"))\n\n\n\n\n\n\nSummary\nAs expected, the models using the adstock transformation did a better job at predicting future sales. The multi/single adstock models seemed to perform about the same here with the single slightly edging out the multi model."
  },
  {
    "objectID": "posts/R_CountChickens/index.html",
    "href": "posts/R_CountChickens/index.html",
    "title": "Count Your Chickens!",
    "section": "",
    "text": "Introduction\nWe finally opened up the board game Count your Chickens, which my daughter received for Christmas. We played a game that we won fairly easily, which got me to thinking how easy is this game? From just our one game it seemed like it was pretty hard to lose so I decided to try to write a program that would simulate the game play and thus allow me to estimate the probability of winning.\n\n\nThe game rules\nObjective: Get to the end of the path with &gt;= 40 chicks in the coop.\nGame play:\n\nSpin the spinner\n\nIf spin == fox\n\nremove 1 chick from the coop, next player goes.\n\nIf spin != fox\n\nmove your game piece to the next space on the board that contains the item you spun. If there is no matching item, move to the end of the path.\nThe number number of spaces you moved along the board is the number of chicks you pick up along the way.\nIf you land on a bonus square, you get an bonus chick.\nNext player goes.\n\n\n\nWriting R function to play a round of the game and record the results\n\nplayGame &lt;-\nfunction(){\n \n  #Create the spinner, game board, and bonus vectors to reference\n  spinner &lt;- c(\"sheep\",\"cow\",\"dog\",\"pig\",\"tractor\",\"fox\")\n  board &lt;- c(\"empty\", \"sheep\", \"pig\", \"tractor\", \"cow\", \"dog\", \"pig\", \"cow\", \"dog\", \"sheep\", \"tractor\",\"empty\", \"cow\", \"pig\",\"empty\",\"empty\",\"empty\",\"tractor\", \"empty\", \"tractor\", \"dog\", \"sheep\",\"cow\", \"dog\", \"pig\", \"tractor\", \"empty\", \"sheep\", \"cow\", \"empty\",\"empty\", \"tractor\", \"pig\",\"sheep\", \"dog\", \"empty\", \"sheep\", \"cow\", \"pig\", \"end\")\n\n  bonus &lt;- rep(0,40)\n  bonus[c(4,8,22,35,39)] &lt;- 1\n\n  #Initialize some iteration variables to store counts to output\n  totalSpaces &lt;- 0;\n  numChicks &lt;- 0\n  totalSpins &lt;- 0\n  numFox &lt;- 0\n  stolenChicks &lt;- 0\n  numBonus &lt;- 0\n\n\n  while(TRUE){\n    #spin the spinner\n    c_spin &lt;- sample(spinner,1)\n    totalSpins &lt;- totalSpins + 1\n \n    #Check if the player spun fox, if so steal a chick if there is one to steal and spin again.\n    if(c_spin == \"fox\"){\n      numChicks &lt;- max(0,numChicks-1)\n      numFox &lt;- numFox + 1\n      next\n    }\n   \n    #find how many spaces it is till the next item spun on the gameboard.\n    cut &lt;- min(which(board==c_spin))\n \n    #If the board ends before the next item spun shows up, it will retun 'Inf' so we'll\n    #just set it to be the remaining number of spaces left on the board.\n    if(cut == \"Inf\") cut &lt;- length(board)\n    totalSpaces &lt;- totalSpaces + cut\n \n    if(bonus[totalSpaces]==1) numBonus &lt;- numBonus + 1\n    numChicks &lt;- numChicks + cut + bonus[totalSpaces]\n    board &lt;- board[-c(1:cut)]\n    if(length(board) == 0){ break}\n  }\n  data.frame(chicks = numChicks, foxes = numFox, bonus = numBonus, spins = totalSpins)\n}\n\nPlay 10,000 games and record the results\n\nregisterDoParallel(cores=7)\nstacked &lt;- foreach(i=1:10000, .combine = rbind) %dopar% playGame()\n\nPlot the results\n\nwin_prop &lt;- round(mean(stacked$chicks &gt;= 40),2)*100\n\nstacked |&gt;\n  mutate(cc = if_else(chicks &gt;= 40,str_glue(\"Won ({win_prop}%)\"),str_glue(\"Lost ({100-win_prop}%)\"))) |&gt;\n  #count(chicks) |&gt;\n  ggplot(aes(x = chicks, fill = cc)) +\n  geom_histogram(binwidth = 1, color = \"black\") +\n  scale_fill_manual(values = c(\"#8795E8\", \"#FF7FD9\")) +\n  labs(x = \"Total Number of Chicks at end of game\",\n       y = \"Number of Games\",\n       title = str_glue(\"Results of 10,000 games played.\"),\n       title2 = \"sdf\",\n       fill = \"Results\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Milt-Hoke.com",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nOct 12, 2022\n\n\nCount Your Chickens!\n\n\nR, Just for Fun\n\n\n\n\nNov 2, 2021\n\n\nA tunable adstock step function\n\n\nR, Tidymodels\n\n\n\n\nOct 28, 2020\n\n\nScraping web data with R and Docker\n\n\nR, Docker, Webscraping\n\n\n\n\n\n\nNo matching items"
  }
]