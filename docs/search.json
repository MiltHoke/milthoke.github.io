[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Milt-Hoke.com",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\nJan 1, 2024\n\n\nUsing renv to create reproducible posts.\n\n\nData Science/Programming\n\n\n\n\nOct 12, 2022\n\n\nCount Your Chickens!\n\n\nData Science/Programming\n\n\n\n\nOct 28, 2020\n\n\nScraping web data with R and Docker\n\n\nData Science/Programming\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/webscraping/index.html",
    "href": "posts/webscraping/index.html",
    "title": "Scraping web data with R and Docker",
    "section": "",
    "text": "Introduction\nI’ve spent a considerable amount of time sifting through tutorials on how to scrape data from the web. Both R and Python offer tools to easily parse html data but so far the only easy solution I’ve found to scrape dynamic data rendered though JS (maybe it’s JS?, either way it shows up magicially on the screen but isn’t in the html to parse) is in R.\nFor this little project we will be recording the building capacity counts for the UW Madison Rec. Inside this site there are two pages we’ll want to scrape. The first page is the overall building capactiy at https://services.recwell.wisc.edu/FacilityOccupancy and a more granular look at specific area capactiy in https://recwell.wisc.edu/liveusage/.\nLastly, I don’t remember what I’ve installed onto my computer to make this all run, but you don’t need to worry about that because we’ll also put together a docker image you can build to easily run both of the examples below.\n\n\nUsing rvest (EASY MODE)\nFirst we’ll start with scraping overall building occupancy. For this we just need to grab the HTML from the URL https://services.recwell.wisc.edu/FacilityOccupancy and strip out the numbers that we want using their xpaths.\nlibrary(rvest)\nlibrary(tidyverse)\nurlPath &lt;- 'https://services.recwell.wisc.edu/FacilityOccupancy'\nhtml &lt;- read_html(urlPath) \n\n#output of html\n&gt; html\n{html_document}  \n&lt;html lang=\"en-US\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\"&gt;\\n&lt;link rel=\"icon\" href=\"~/favicon.ico\"&gt;\\n&lt;meta charset=\"utf-8\"&gt;\\n&lt;meta name=\"v \n[2] &lt;body&gt;\\r\\n    &lt;div role=\"complementary\" aria-label=\"skip to main content\"&gt;&lt;a id=\"skipLink\" class=\"skip-main\" tabindex=\"1\" onclick=\"$('#mainContent').find( \nNow we have a list of html text stored in the variable html (output shown above). Inside of html are the two numbers we want to extract: Current Occupancy and max Occupancy. First you need to find the xpaths (or something similiar) for each occupancy value you want to pull out. If you don’t know how to do this just search for ‘finding xpath in chrome’ and you’re sure to find something. Once you have the xpath for the values you want the rest is pretty straightforward.\noccupancy &lt;- \n  html %&gt;%\n  html_nodes(xpath='//*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[3]/strong | //*[@id=\"occupancy-65cc2f42-1ca8-4afe-bf5a-990b1a9e4111\"]/div[2]/p[1]/strong') %&gt;%\n  html_text() %&gt;% \n  str_replace(\"%\",\"\") %&gt;% \n  as.numeric() \n  \n&gt; occupancy\n[1] 222  73\nIn the code above, html_nodes() will extract the values we want from the html based on the xpaths we gave it to look for. This value will still have html tags on it (e.g. &lt;strong&gt; 222 &lt;/strong&gt;) which we can remove using the html_text() function. From there it’s just some simple cleanup to remove special characters with str_replace() and convert them from character into numeric using as.numeric().\nFinally we can take our vector and turn it into a dataframe for additional manipulation.\ndf &lt;- data.frame(max_occupancy  = occupancy[1], current_occupancy = occupancy[2]/100, pulled = Sys.time())\n&gt; df\n  max_occupancy current_occupancy              pulled\n1           222              0.73 2020-10-28 19:35:19\n\n\nUsing pagedown and pdftools\nOK, now lets wrap this up and head over to https://recwell.wisc.edu/liveusage/, grab a few xpaths and be good to go. Unfortunatly this one isn’t quite as easy.\nurlPath &lt;- 'https://recwell.wisc.edu/liveusage/'\nhtml &lt;- read_html(urlPath)\n\nhtml %&gt;%\nhtml_nodes(xpath='//*[@id=\"nick\"]/div/div[2]/div[2]/div/div/div[1]/div/div[2]/p[2]/span[1]') %&gt;%\n  html_text()\n  \n[1] \"0\"\nThe code above will return 0 instead of 18 which is the correct value at the time I ran this example. I believe 0 is a placeholder and the actual value gets updated at a later time. Either way, it doesn’t look like the method we used above will work for us so we’ll have to shift gears a bit.\nThe most reliable way I’ve found to scrape this type of data is to use pagedown to print the page to a pdf. Then use pdftools to read the pdf back into R. Then use some regular expression magic to parse everything you need.\n#Set some variables\nlibrary(pdftools)\nlibrary(tidyverse)\nurlPath &lt;- 'https://recwell.wisc.edu/liveusage/'\npdfPath &lt;- \"./rec.pdf\"\n\n#Print out the webpage to pdf using pagedown\npagedown::chrome_print(urlPath,pdfPath,extra_args = '--no-sandbox')\n\n#Read in the pdf again using pdftools (and some stringr magic)\nx &lt;- str_squish(unlist(str_split(str_flatten(pdf_text(pdfPath)),\"\\n\")))\n\n#Use regular expressions to parse out the data we want to keep\nvalues_bool &lt;- str_detect(x,\"^Updated (.*?) \\\\d+ / \\\\d+\")\nlabel_bool &lt;- values_bool[c(2:length(values_bool),FALSE)]\n\n#Turn it into a dataframe for additional manipulation.\ndata.frame(location = x[label_bool],values = x[values_bool])\n\n\nMaking it easy with Docker\nYou can build your own image using the Dockerfile code below, or pull mine from docker hub at mjholt02/pagedown.\nFROM r-base\n\nRUN apt-get update -qq && apt-get -y install libssl-dev \\\n    chromium libcurl4-openssl-dev \\ \n    libxml2-dev libpoppler-cpp-dev libpq-dev\n\nRUN install2.r RPostgres pagedown pdftools tidyverse \\ \n    rvest\nFrom there is as easy as starting up an interactive container using docker run --rm -it mjholt02/pagedown or winpty docker run --rm -it mjholt02/pagedown if you’re using git bash."
  }
]