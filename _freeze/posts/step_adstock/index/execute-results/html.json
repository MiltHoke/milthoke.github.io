{
  "hash": "f23084c9822512cd77431d1d1fa5b7ba",
  "result": {
    "markdown": "---\ntitle: \"A tunable adstock step function\"\nauthor: \"Milt\"\ndate: \"2021-11-02\"\ncategories: [\"R\", \" Tidymodels\"]\n---\n\n::: {.cell}\n\n:::\n\n\n### Introduction\n\nI've been using the Tidymodels framework quite a bit lately and was interested in creating a tunable adstock function to search for the ideal rate without deviating from the Tidymodels framework. The functions below are pretty much just a rework of the ones provided in the documentation but extending it to include a tunable function input.\n\n### Creating the functions\n\nBelow are the functions you'll need to prep, bake, and tune your step function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n### Specify the function to calculate Adstock given a spend vector\ncalcAdstock <-function(x,w,args=NULL){\n  return(as.numeric(stats::filter(x=x,filter=w,method=\"recursive\")))\n}\n\nstep_adstock <- \nfunction(recipe, ..., role=NA, trained=FALSE, skip=FALSE, \n  id=rand_id(\"adstock\"), my_vector=NULL, w=0){\n  \n  terms = ellipse_check(...)\n  \n  add_step(\n    recipe,\n    step_adstock_new(\n      terms=terms,\n      trained=trained,\n      role=role,\n      skip=skip,\n      id=id,\n      my_vector=my_vector,\n      w=w)\n  )\n}\n                         \nstep_adstock_new <- \nfunction(terms, role, trained, my_vector,skip, id, w){\n  step(\n    subclass = \"adstock\",\n    terms=terms,\n    role=role,\n    trained=trained,\n    my_vector=my_vector,\n    skip=skip,\n    id=id,\n    w=w)\n}    \n\nprep.step_adstock <- \nfunction(x,training,info=NULL, ...){\n  col_names <- recipes_eval_select(x$terms, training, info)\n  \n  my_vector <- purrr::map(training[, col_names],calcAdstock, w=x$w)\n  \n  step_adstock_new(\n    terms = x$terms,\n    trained=TRUE,\n    role=x$role,\n    my_vector = my_vector,\n    skip = x$skip,\n    id = x$id,\n    w=x$w)\n}\n\nbake.step_adstock <- \nfunction(object, new_data, ...){\n  vars <- names(object$my_vector)\n  \n  new_data[,vars]<- \n    purrr::map2_dfc(new_data[,vars],object$my_vector, calcAdstock, w=object$w)\n  \n  tibble::as_tibble(new_data)\n}\n\ntunable.step_adstock <- \nfunction(x, ...){\n  tibble::tibble(\n    name = c(\"w\"),\n    call_info = list(list(pkg=\"dials\",fun=\"mixture\",range=c(0,1))),\n    source = \"recipe\",\n    component = \"step_adstock\",\n    component_id = x$id)\n}\n```\n:::\n\n\n### Example\n\nFirst step is to simulate some data so our dependent variable (sales) is related to the lagged values of tv and radio spend.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"pacman\")\nset.seed(123)\npacman::p_load(tidyverse, tidymodels,lubridate,doParallel)\n\ndateVector <- \n  seq.Date(as.Date(\"2018-01-01\"),as.Date(\"2021-12-31\"),1)\n\ndf <- \n  tibble(date = dateVector, \n         tv = rnorm(length(dateVector),100,20), \n         radio = rnorm(length(dateVector),50,5)) %>%\n  mutate(sales = round((lag(tv,1)*.075) + (lag(tv,2)*0.0375) + \n                 (lag(radio,1) *.01 )) ) %>%\n  drop_na()\n\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  date          tv radio sales\n  <date>     <dbl> <dbl> <dbl>\n1 2018-01-03 131.   53.2    11\n2 2018-01-04 101.   52.1    14\n3 2018-01-05 103.   49.0    13\n4 2018-01-06 134.   49.6    12\n5 2018-01-07 109.   53.4    14\n6 2018-01-08  74.7  50.9    14\n```\n:::\n:::\n\n\nNext we split into our training/test sets. In this case we need to incorporate lagged values so when we make predictions on the test set we have the appropriate spend history to build up our adstock transformation properly for the first few observations.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_split <- initial_time_split(df,lag=8)\ntrain <- training(my_split)\ntest <- testing(my_split)\n\nmy_rs <- \n  sliding_period(train, date, period = \"month\", \n                 lookback = 24, assess_stop = 4)\n```\n:::\n\n\nUsing the tidymodels framework, lets setup, run/tune, and fit our 3 models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel()\n\nmdl_linear <- \n  linear_reg() \n\nr_base <- \n  recipe(sales ~ tv + radio, data = train) \n\nr_single <- r_base %>%\n  step_adstock(c('tv','radio'), w = tune('w1')) \n\nr_multi <- r_base %>%\n  step_adstock('tv', w = tune('w1')) %>%\n  step_adstock('radio', w = tune('w2')) \n\n\nwkflw <-\n  workflow() %>%\n  add_model(mdl_linear)\n\nwkflw_base   <- wkflw %>% add_recipe(r_base)\nwkflw_single <- wkflw %>% add_recipe(r_single)\nwkflw_multi  <- wkflw %>% add_recipe(r_multi)\n\nfit_base <- \n  wkflw_base %>%\n  last_fit(my_split)\n\n#single adstock \nwkflw_single_t <- \nwkflw_single %>%\n  tune_grid(my_rs,grid=15)\n\nfit_single <- \nwkflw_single %>%\n  finalize_workflow(select_best(wkflw_single_t, metric = \"rmse\")) %>%\n  last_fit(my_split) \n\n#multi adstock \nwkflw_multi_t <- \nwkflw_multi %>%\n  tune_grid(my_rs,grid=15)\n\nfit_multi <- \nwkflw_multi %>%\n  finalize_workflow(select_best(wkflw_multi_t, metric = \"rmse\")) %>%\n  last_fit(my_split) \n\nfinal_base <- extract_workflow(fit_base)\nfinal_single <- extract_workflow(fit_single)\nfinal_multi <- extract_workflow(fit_multi)\n```\n:::\n\n\nExamine our model metrics and the tuned adstock rates.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nselect_best(wkflw_single_t, metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 2\n     w1 .config              \n  <dbl> <chr>                \n1 0.722 Preprocessor11_Model1\n```\n:::\n\n```{.r .cell-code}\nselect_best(wkflw_multi_t, metric = \"rmse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n     w1    w2 .config              \n  <dbl> <dbl> <chr>                \n1 0.802 0.810 Preprocessor11_Model1\n```\n:::\n\n```{.r .cell-code}\ncollect_metrics(fit_base)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard    1.68     Preprocessor1_Model1\n2 rsq     standard    0.000614 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\ncollect_metrics(fit_single)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       1.42  Preprocessor1_Model1\n2 rsq     standard       0.293 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\ncollect_metrics(fit_multi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       1.42  Preprocessor1_Model1\n2 rsq     standard       0.286 Preprocessor1_Model1\n```\n:::\n:::\n\n\nPredict, summarize, and plot our predictions for each model. In the plot below I'll summarize by quarter just to make it easier to visually see the differences between the models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_plot <- test\n\ntest_plot$base <- predict(final_base,test_plot) %>% pull()\ntest_plot$single <- predict(final_single,test_plot) %>% pull()\ntest_plot$multi <- predict(final_multi,test_plot) %>% pull()\n\ntest_plot %>%\n  select(date, sales:multi) %>%\n  pivot_longer(cols = -date) %>%\n  mutate(date = floor_date(date,\"quarter\")) %>%\n  group_by(date,name) %>%\n  summarise(value = sum(value)) %>%\n  filter(date >= \"2021-01-01\") %>%\n  ggplot(aes(x=date, y=value, color = name)) + geom_line(linewidth=1.5) +\n  labs(x=NULL, y = \"Sales\", color = \"Model\") + \n  scale_color_manual(values=c(\"red\", \"blue\", \"black\", \"orange\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=1152}\n:::\n:::\n\n\n### Summary\n\nAs expected, the models using the adstock transformation did a better job at predicting future sales. The multi/single adstock models seemed to perform about the same here with the single slightly edging out the multi model.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}